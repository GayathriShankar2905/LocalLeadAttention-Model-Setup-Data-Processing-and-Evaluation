{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Nb0r00EXPii",
        "outputId": "5bf04bb8-a42a-4838-963c-13362adb4379"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'LocalLeadAttention'...\n",
            "remote: Enumerating objects: 31, done.\u001b[K\n",
            "remote: Counting objects: 100% (31/31), done.\u001b[K\n",
            "remote: Compressing objects: 100% (29/29), done.\u001b[K\n",
            "remote: Total 31 (delta 5), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (31/31), 7.41 MiB | 4.38 MiB/s, done.\n",
            "Resolving deltas: 100% (5/5), done.\n",
            "/content/LocalLeadAttention/LocalLeadAttention\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/cph-cachet/LocalLeadAttention.git\n",
        "%cd LocalLeadAttention\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8h7ey3DXXlJ",
        "outputId": "e07a0b1f-32a9-4f03-a786-cd5ac4f9421b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-multilearn in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy scipy scikit-learn joblib pandas tabulate tqdm scikit-multilearn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "AHqUeRvEXlVk"
      },
      "outputs": [],
      "source": [
        "def load_challenge_data(filename):\n",
        "    # Example: Loading ECG signals and metadata\n",
        "    with open(filename, 'r') as f:\n",
        "        # Parse data and return signals, labels, etc.\n",
        "        data = f.read()\n",
        "    return data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CcCWrKAdX9QE",
        "outputId": "83352caf-a2cd-44b9-c70d-226c114121c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "📥 Downloading RECORDS file...\n",
            "✅ RECORDS file downloaded!\n",
            "✅ Successfully downloaded: 102.dat\n",
            "✅ Successfully downloaded: 103.dat\n",
            "✅ Successfully downloaded: 101.dat\n",
            "✅ Successfully downloaded: 100.dat\n",
            "✅ Successfully downloaded: 102.hea\n",
            "✅ Successfully downloaded: 104.dat\n",
            "✅ Successfully downloaded: 103.hea\n",
            "✅ Successfully downloaded: 100.hea\n",
            "✅ Successfully downloaded: 101.hea\n",
            "✅ Successfully downloaded: 104.hea\n",
            "✅ Successfully downloaded: 102.atr\n",
            "✅ Successfully downloaded: 103.atr\n",
            "✅ Successfully downloaded: 100.atr\n",
            "✅ Successfully downloaded: 101.atr\n",
            "✅ Successfully downloaded: 104.atr\n",
            "✅ Successfully downloaded: 105.dat\n",
            "✅ Successfully downloaded: 107.dat\n",
            "✅ Successfully downloaded: 106.dat\n",
            "✅ Successfully downloaded: 108.dat\n",
            "✅ Successfully downloaded: 105.hea\n",
            "✅ Successfully downloaded: 109.dat\n",
            "✅ Successfully downloaded: 107.hea\n",
            "✅ Successfully downloaded: 106.hea\n",
            "✅ Successfully downloaded: 108.hea\n",
            "✅ Successfully downloaded: 105.atr\n",
            "✅ Successfully downloaded: 109.hea\n",
            "✅ Successfully downloaded: 107.atr\n",
            "✅ Successfully downloaded: 106.atr\n",
            "✅ Successfully downloaded: 108.atr\n",
            "✅ Successfully downloaded: 109.atr\n",
            "✅ Successfully downloaded: 111.dat\n",
            "✅ Successfully downloaded: 114.dat\n",
            "✅ Successfully downloaded: 112.dat\n",
            "✅ Successfully downloaded: 113.dat\n",
            "✅ Successfully downloaded: 111.hea\n",
            "✅ Successfully downloaded: 114.hea\n",
            "✅ Successfully downloaded: 115.dat\n",
            "✅ Successfully downloaded: 113.hea\n",
            "✅ Successfully downloaded: 112.hea\n",
            "✅ Successfully downloaded: 111.atr\n",
            "✅ Successfully downloaded: 114.atr\n",
            "✅ Successfully downloaded: 115.hea\n",
            "✅ Successfully downloaded: 113.atr\n",
            "✅ Successfully downloaded: 112.atr\n",
            "✅ Successfully downloaded: 115.atr\n",
            "✅ Successfully downloaded: 116.dat\n",
            "✅ Successfully downloaded: 117.dat\n",
            "✅ Successfully downloaded: 121.dat\n",
            "✅ Successfully downloaded: 118.dat\n",
            "✅ Successfully downloaded: 119.dat\n",
            "✅ Successfully downloaded: 116.hea\n",
            "✅ Successfully downloaded: 117.hea\n",
            "✅ Successfully downloaded: 121.hea\n",
            "✅ Successfully downloaded: 118.hea\n",
            "✅ Successfully downloaded: 119.hea\n",
            "✅ Successfully downloaded: 116.atr\n",
            "✅ Successfully downloaded: 117.atr\n",
            "✅ Successfully downloaded: 121.atr\n",
            "✅ Successfully downloaded: 118.atr\n",
            "✅ Successfully downloaded: 119.atr\n",
            "✅ Successfully downloaded: 122.dat\n",
            "✅ Successfully downloaded: 123.dat\n",
            "✅ Successfully downloaded: 124.dat\n",
            "✅ Successfully downloaded: 122.hea\n",
            "✅ Successfully downloaded: 200.dat\n",
            "✅ Successfully downloaded: 201.dat\n",
            "✅ Successfully downloaded: 123.hea\n",
            "✅ Successfully downloaded: 124.hea\n",
            "✅ Successfully downloaded: 122.atr\n",
            "✅ Successfully downloaded: 200.hea\n",
            "✅ Successfully downloaded: 201.hea\n",
            "✅ Successfully downloaded: 123.atr\n",
            "✅ Successfully downloaded: 124.atr\n",
            "✅ Successfully downloaded: 200.atr\n",
            "✅ Successfully downloaded: 201.atr\n",
            "✅ Successfully downloaded: 202.dat\n",
            "✅ Successfully downloaded: 202.hea\n",
            "✅ Successfully downloaded: 202.atr\n",
            "✅ Successfully downloaded: 203.dat\n",
            "✅ Successfully downloaded: 207.dat\n",
            "✅ Successfully downloaded: 205.dat\n",
            "✅ Successfully downloaded: 208.dat\n",
            "✅ Successfully downloaded: 203.hea\n",
            "✅ Successfully downloaded: 207.hea\n",
            "✅ Successfully downloaded: 205.hea\n",
            "✅ Successfully downloaded: 208.hea\n",
            "✅ Successfully downloaded: 203.atr\n",
            "✅ Successfully downloaded: 207.atr\n",
            "✅ Successfully downloaded: 205.atr\n",
            "✅ Successfully downloaded: 208.atr\n",
            "✅ Successfully downloaded: 209.dat\n",
            "✅ Successfully downloaded: 209.hea\n",
            "✅ Successfully downloaded: 209.atr\n",
            "✅ Successfully downloaded: 210.dat\n",
            "✅ Successfully downloaded: 212.dat\n",
            "✅ Successfully downloaded: 213.dat\n",
            "✅ Successfully downloaded: 210.hea\n",
            "✅ Successfully downloaded: 212.hea\n",
            "✅ Successfully downloaded: 214.dat\n",
            "✅ Successfully downloaded: 213.hea\n",
            "✅ Successfully downloaded: 210.atr\n",
            "✅ Successfully downloaded: 212.atr\n",
            "✅ Successfully downloaded: 214.hea\n",
            "✅ Successfully downloaded: 213.atr\n",
            "✅ Successfully downloaded: 214.atr\n",
            "✅ Successfully downloaded: 215.dat\n",
            "✅ Successfully downloaded: 215.hea\n",
            "✅ Successfully downloaded: 215.atr\n",
            "✅ Successfully downloaded: 217.dat\n",
            "✅ Successfully downloaded: 219.dat\n",
            "✅ Successfully downloaded: 217.hea\n",
            "✅ Successfully downloaded: 219.hea\n",
            "✅ Successfully downloaded: 220.dat\n",
            "✅ Successfully downloaded: 221.dat\n",
            "✅ Successfully downloaded: 217.atr\n",
            "✅ Successfully downloaded: 219.atr\n",
            "✅ Successfully downloaded: 220.hea\n",
            "✅ Successfully downloaded: 221.hea\n",
            "✅ Successfully downloaded: 220.atr\n",
            "✅ Successfully downloaded: 221.atr\n",
            "✅ Successfully downloaded: 222.dat\n",
            "✅ Successfully downloaded: 222.hea\n",
            "✅ Successfully downloaded: 222.atr\n",
            "✅ Successfully downloaded: 223.dat\n",
            "✅ Successfully downloaded: 228.dat\n",
            "✅ Successfully downloaded: 223.hea\n",
            "✅ Successfully downloaded: 228.hea\n",
            "✅ Successfully downloaded: 223.atr\n",
            "✅ Successfully downloaded: 230.dat\n",
            "✅ Successfully downloaded: 231.dat\n",
            "✅ Successfully downloaded: 228.atr\n",
            "✅ Successfully downloaded: 231.hea\n",
            "✅ Successfully downloaded: 230.hea\n",
            "✅ Successfully downloaded: 230.atr\n",
            "✅ Successfully downloaded: 231.atr\n",
            "✅ Successfully downloaded: 232.dat\n",
            "✅ Successfully downloaded: 232.hea\n",
            "✅ Successfully downloaded: 232.atr\n",
            "✅ Successfully downloaded: 233.dat\n",
            "✅ Successfully downloaded: 233.hea\n",
            "✅ Successfully downloaded: 234.dat\n",
            "✅ Successfully downloaded: 233.atr\n",
            "✅ Successfully downloaded: 234.hea\n",
            "✅ Successfully downloaded: 234.atr\n",
            "✅ Download process completed!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import requests\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "\n",
        "# ✅ Base URL for the MIT-BIH Arrhythmia Database\n",
        "BASE_URL = \"https://physionet.org/files/mitdb/1.0.0/\"\n",
        "\n",
        "# ✅ Directory to save downloaded files\n",
        "SAVE_DIR = \"./mitdb/records/\"\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "\n",
        "# ✅ URL of the RECORDS file (list of all record names)\n",
        "records_file_url = BASE_URL + \"RECORDS\"\n",
        "records_file_path = os.path.join(SAVE_DIR, \"RECORDS\")\n",
        "\n",
        "# ✅ Download the RECORDS file if it doesn't already exist\n",
        "if not os.path.exists(records_file_path):\n",
        "    print(\"📥 Downloading RECORDS file...\")\n",
        "    response = requests.get(records_file_url)\n",
        "    response.raise_for_status()\n",
        "    with open(records_file_path, \"w\") as f:\n",
        "        f.write(response.text)\n",
        "    print(\"✅ RECORDS file downloaded!\")\n",
        "\n",
        "# ✅ Read the list of valid records from the RECORDS file\n",
        "with open(records_file_path, \"r\") as f:\n",
        "    valid_records = [line.strip() for line in f.readlines()]\n",
        "\n",
        "# ✅ Choose the first 20 records\n",
        "selected_records = valid_records[:100]  # First 20 records\n",
        "\n",
        "# ✅ Function to download .dat, .hea, and .atr files for a record\n",
        "def download_record(record_name):\n",
        "    \"\"\"Download .dat, .hea, and .atr files for the given record.\"\"\"\n",
        "    for ext in [\".dat\", \".hea\", \".atr\"]:\n",
        "        file_url = f\"{BASE_URL}{record_name}{ext}\"\n",
        "        file_path = os.path.join(SAVE_DIR, f\"{record_name}{ext}\")\n",
        "\n",
        "        # Skip download if the file already exists\n",
        "        if os.path.exists(file_path):\n",
        "            print(f\"✅ Already downloaded: {record_name}{ext}\")\n",
        "            continue\n",
        "\n",
        "        try:\n",
        "            response = requests.get(file_url, stream=True)\n",
        "            response.raise_for_status()\n",
        "            with open(file_path, \"wb\") as file:\n",
        "                for chunk in response.iter_content(chunk_size=1024):\n",
        "                    file.write(chunk)\n",
        "            print(f\"✅ Successfully downloaded: {record_name}{ext}\")\n",
        "        except requests.exceptions.RequestException as e:\n",
        "            print(f\"❌ Failed to download {record_name}{ext}: {e}\")\n",
        "\n",
        "# ✅ Use multi-threading to download selected records\n",
        "with ThreadPoolExecutor(max_workers=5) as executor:\n",
        "    executor.map(download_record, selected_records)\n",
        "\n",
        "print(\"✅ Download process completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Z-mXvoWYnaF",
        "outputId": "e6cfdb30-1f6b-488d-c070-e6a9d2fcf26b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wfdb in /usr/local/lib/python3.11/dist-packages (4.2.0)\n",
            "Requirement already satisfied: matplotlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from wfdb) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.26.4 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.26.4)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.2.3)\n",
            "Requirement already satisfied: requests>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from wfdb) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (1.13.1)\n",
            "Requirement already satisfied: soundfile>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from wfdb) (0.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.2.2->wfdb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->wfdb) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.8.1->wfdb) (2025.1.31)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.10.0->wfdb) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.10.0->wfdb) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.2.2->wfdb) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install wfdb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cv84yg10YUon",
        "outputId": "ba0a9bd9-bc02-48c2-f341-b43d884ebacd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signal shape: (650000, 2)\n"
          ]
        }
      ],
      "source": [
        "import wfdb\n",
        "\n",
        "# Example: Load a record\n",
        "record = wfdb.rdrecord(\"./mitdb/records/100\")\n",
        "signal = record.p_signal  # ECG signal\n",
        "print(\"Signal shape:\", signal.shape)  # Typically [N_samples, N_leads]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGgwxoalYmAp",
        "outputId": "8e4c4eb5-4431-4da9-95a6-30b7afccf9d3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resampled shape: (902777, 2)\n"
          ]
        }
      ],
      "source": [
        "from scipy.signal import resample\n",
        "\n",
        "def resample_signal(signal, original_rate=360, target_rate=500):\n",
        "    \"\"\"Resample the signal to the desired rate.\"\"\"\n",
        "    num_samples = int(signal.shape[0] * target_rate / original_rate)\n",
        "    resampled_signal = resample(signal, num_samples, axis=0)  # Resample along the time axis\n",
        "    return resampled_signal\n",
        "\n",
        "# Example\n",
        "resampled_signal = resample_signal(signal, original_rate=360, target_rate=500)\n",
        "print(\"Resampled shape:\", resampled_signal.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gPeWRc12YxsP",
        "outputId": "66ff9c44-633b-4a1f-e0cd-81e05ea73447"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Normalized shape: (902777, 2)\n"
          ]
        }
      ],
      "source": [
        "def normalize_signal(signal):\n",
        "    \"\"\"Normalize each lead (channel) independently.\"\"\"\n",
        "    normalized_signal = (signal - signal.mean(axis=0)) / signal.std(axis=0)\n",
        "    return normalized_signal\n",
        "\n",
        "# Example\n",
        "normalized_signal = normalize_signal(resampled_signal)  # Apply to resampled signal\n",
        "print(\"Normalized shape:\", normalized_signal.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "wj6hGoxpJSB1"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jH_1HiUhYyFx",
        "outputId": "5959ea73-5ccc-4987-e633-33b91138b4e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segments shape: (501, 1800, 2)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "def segment_signal(signal, window_size=1800, step_size=1800):\n",
        "    \"\"\"Segment the signal into overlapping/non-overlapping windows.\"\"\"\n",
        "    segments = []\n",
        "    for start in range(0, signal.shape[0] - window_size + 1, step_size):\n",
        "        segments.append(signal[start:start + window_size])\n",
        "    return np.array(segments)\n",
        "\n",
        "# Example\n",
        "window_size = 1800  # 5 seconds at 360 Hz\n",
        "segments = segment_signal(normalized_signal, window_size=window_size, step_size=window_size)\n",
        "print(\"Segments shape:\", segments.shape)  # Expected: (num_windows, 1800, 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define normal and arrhythmia label mapping\n",
        "normal_labels = {'N'}  # Normal beats\n",
        "arrhythmia_labels = set(unique_labels) - normal_labels  # All other beats are arrhythmias\n",
        "\n",
        "print(\"Arrhythmia labels:\", arrhythmia_labels)  # Verify arrhythmia labels\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBBq2CnBf1aQ",
        "outputId": "f02cea39-c46f-4d4e-c046-b32186e5e406"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arrhythmia labels: {'A', 'V', '+'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def assign_labels_to_segments(segments, sample_indices, labels, window_size=1800):\n",
        "    \"\"\"\n",
        "    Assigns a binary label (0: Normal, 1: Arrhythmia) to each segment based on annotations.\n",
        "    \"\"\"\n",
        "    segment_labels = []\n",
        "\n",
        "    for i in range(len(segments)):\n",
        "        start = i * window_size  # Segment start time\n",
        "        end = start + window_size  # Segment end time\n",
        "\n",
        "        # ✅ Find indices where the sample annotations fall within this segment\n",
        "        mask = (sample_indices >= start) & (sample_indices < end)\n",
        "        segment_annotations = np.array(labels)[mask]  # Extract labels for this segment\n",
        "\n",
        "        # ✅ Assign label: 1 if any arrhythmia is present, else 0 (normal)\n",
        "        segment_label = 1 if any(label in arrhythmia_labels for label in segment_annotations) else 0\n",
        "        segment_labels.append(segment_label)\n",
        "\n",
        "    return np.array(segment_labels)\n",
        "\n",
        "# ✅ Compute segment labels\n",
        "segment_labels = assign_labels_to_segments(segments, np.array(sample_indices), np.array(labels), window_size=window_size)\n",
        "\n",
        "print(\"Segment Labels:\", segment_labels[:10])  # Check first 10 segment labels\n",
        "print(\"Type of first label:\", type(segment_labels[0]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsCcdcIOf7Ge",
        "outputId": "5dd4ec46-57b8-4ca1-aa2e-04f58f13a274"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Segment Labels: [1 1 0 0 0 0 0 0 0 0]\n",
            "Type of first label: <class 'numpy.int64'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "import numpy as np\n",
        "\n",
        "# Convert to NumPy array\n",
        "segment_labels_int = np.array(segment_labels, dtype=np.int64)\n",
        "\n",
        "# ✅ Reshape segments to 2D (flattened representation)\n",
        "num_samples, window_size, num_leads = segments.shape\n",
        "segments_reshaped = segments.reshape(num_samples, -1)  # Shape: (num_samples, window_size * num_leads)\n",
        "\n",
        "# ✅ Apply SMOTE only to the minority class (arrhythmia)\n",
        "smote = SMOTE(sampling_strategy=0.5, random_state=42)  # Adjust as needed\n",
        "segments_resampled, labels_resampled = smote.fit_resample(segments_reshaped, segment_labels_int)\n",
        "\n",
        "# ✅ Reshape back to original shape (3D) for model training\n",
        "segments_resampled = segments_resampled.reshape(-1, window_size, num_leads)  # Shape: (new_samples, window_size, num_leads)\n",
        "\n",
        "# ✅ Print new class distribution\n",
        "unique, counts = np.unique(labels_resampled, return_counts=True)\n",
        "print(\"New balanced class distribution:\", dict(zip(unique, counts)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JL4ZQB1rgj0K",
        "outputId": "cbb86605-27da-41c9-a2e0-518d16dfbfc6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New balanced class distribution: {0: 467, 1: 233}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BJ6AyhSjMMcc",
        "outputId": "7f320180-a892-43a7-ffcd-48aca75a4847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution: {0: 467, 1: 34}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "unique, counts = np.unique(segment_labels, return_counts=True)\n",
        "print(\"Class distribution:\", dict(zip(unique, counts)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "SMUV1qZTY1gO"
      },
      "outputs": [],
      "source": [
        "def load_mitdb_data(data_dir):\n",
        "    # Load preprocessed data (signals and labels)\n",
        "    signals = np.load(os.path.join(data_dir, \"signals.npy\"))\n",
        "    labels = np.load(os.path.join(data_dir, \"labels.npy\"))\n",
        "    return signals, labels\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.save(\"mitdb_signals.npy\", segments_resampled)  # Save augmented ECG signals\n",
        "np.save(\"mitdb_labels.npy\", labels_resampled)    # Save corresponding labels\n"
      ],
      "metadata": {
        "id": "zQRLJbgQg_Fl"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "\n",
        "# Define file paths\n",
        "signals_path = \"./mitdb_signals.npy\"\n",
        "labels_path = \"./mitdb_labels.npy\"\n",
        "\n",
        "# ✅ Check if files exist\n",
        "if not os.path.exists(signals_path) or not os.path.exists(labels_path):\n",
        "    print(\"🚨 Saving signals and labels before loading...\")\n",
        "\n",
        "    # Save signals and labels to disk\n",
        "    np.save(signals_path, segments_resampled)  # Save augmented ECG signals\n",
        "    np.save(labels_path, labels_resampled)     # Save corresponding labels\n",
        "    print(\"✅ Files saved successfully!\")\n",
        "\n",
        "# ✅ Load the saved data\n",
        "def load_mitdb_data(data_dir):\n",
        "    \"\"\"Load ECG signals and labels from .npy files.\"\"\"\n",
        "    signals = np.load(os.path.join(data_dir, \"mitdb_signals.npy\"))\n",
        "    labels = np.load(os.path.join(data_dir, \"mitdb_labels.npy\"))\n",
        "    return signals, labels\n",
        "\n",
        "signals, labels = load_mitdb_data(\".\")  # Load signals and labels\n",
        "print(\"✅ Loaded signals shape:\", signals.shape)\n",
        "print(\"✅ Loaded labels shape:\", labels.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmUqSEZ6hCDY",
        "outputId": "d755f012-0953-441b-8abb-d20b04ae8493"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Loaded signals shape: (700, 1800, 2)\n",
            "✅ Loaded labels shape: (700,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dgV308mzY5Nb",
        "outputId": "86746ab2-95dc-494d-e062-cc3ffbb1739c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LocalLeadAttention(\n",
            "  (input_layer): Conv1d(2, 256, kernel_size=(5,), stride=(1,), padding=(2,))\n",
            "  (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (dropout): Dropout(p=0.3, inplace=False)\n",
            "  (attention): MultiheadAttention(\n",
            "    (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
            "  )\n",
            "  (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
            "  (fc1): Linear(in_features=256, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=4, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class LocalLeadAttention(nn.Module):\n",
        "    def __init__(self, input_size, num_classes, num_leads=2, hidden_size=256, num_heads=4, dropout_rate=0.3):\n",
        "        \"\"\"\n",
        "        LocalLeadAttention model for multi-lead ECG classification.\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Length of the input signal (e.g., 1800 time steps).\n",
        "            num_classes (int): Number of output classes (e.g., 4).\n",
        "            num_leads (int): Number of input channels/leads (e.g., 2).\n",
        "            hidden_size (int): Size of the hidden layers.\n",
        "            num_heads (int): Number of attention heads.\n",
        "            dropout_rate (float): Dropout rate for regularization.\n",
        "        \"\"\"\n",
        "        super(LocalLeadAttention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        # Input layer for each lead\n",
        "        self.input_layer = nn.Conv1d(\n",
        "            in_channels=num_leads,\n",
        "            out_channels=hidden_size,\n",
        "            kernel_size=5,\n",
        "            stride=1,\n",
        "            padding=2\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(hidden_size)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "        # Multi-head attention\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            embed_dim=hidden_size,\n",
        "            num_heads=num_heads,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.norm2 = nn.LayerNorm(hidden_size)\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(hidden_size, 64)\n",
        "        self.fc2 = nn.Linear(64, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass of the LocalLeadAttention model.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor of shape (batch_size, time_steps, num_leads).\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output logits of shape (batch_size, num_classes).\n",
        "        \"\"\"\n",
        "        # Reshape input for Conv1d: (batch_size, num_leads, time_steps)\n",
        "        x = x.permute(0, 2, 1)  # Shape: (batch_size, num_leads, time_steps)\n",
        "\n",
        "        # Apply convolution to extract features\n",
        "        x = self.input_layer(x)  # Shape: (batch_size, hidden_size, time_steps)\n",
        "        x = self.dropout(self.norm1(x.permute(0, 2, 1)))  # Normalize and apply dropout\n",
        "        x = x.permute(0, 2, 1)  # Shape: (batch_size, time_steps, hidden_size)\n",
        "\n",
        "        # Apply multi-head attention\n",
        "        attn_output, _ = self.attention(x, x, x)  # Self-attention\n",
        "        x = self.norm2(attn_output + x)  # Add residual connection and normalize\n",
        "\n",
        "        # Global average pooling over time\n",
        "        x = x.mean(dim=1)  # Shape: (batch_size, hidden_size)\n",
        "\n",
        "        # Fully connected layers\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)  # Shape: (batch_size, num_classes)\n",
        "\n",
        "        return x\n",
        "\n",
        "# Define model parameters\n",
        "input_size = 1800  # Signal length per segment\n",
        "num_leads = 2      # Number of ECG leads (channels)\n",
        "num_classes = 4    # Number of output classes ('N', 'A', '+', 'V')\n",
        "\n",
        "# Initialize the model\n",
        "model = LocalLeadAttention(\n",
        "    input_size=input_size,\n",
        "    num_leads=num_leads,\n",
        "    num_classes=num_classes\n",
        ")\n",
        "\n",
        "# Print the model architecture\n",
        "print(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "print(os.listdir(\".\"))  # List files in the current directory\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDqgXeINiNA9",
        "outputId": "7c37f975-1325-4120-8b3a-dc4eee813978"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['.git', 'train_model.py', 'extract_leads_wfdb.py', 'mitdb_labels.npy', 'Dockerfile', 'dx_mapping_scored.csv', 'team_code.py', 'AUTHORS', 'weights.csv', 'model', 'helper_code.py', 'LICENSE', 'model_files', 'test_docker.sh', 'README.md', 'requirements.txt', 'evaluate_model.py', 'test_model.py', 'mitdb', 'dx_mapping_unscored.csv', 'mitdb_signals.npy', 'training.sh', 'model_code.py']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "signals = np.load(\"mitdb_signals.npy\")  # Load signals\n",
        "labels = np.load(\"mitdb_labels.npy\")    # Load labels\n"
      ],
      "metadata": {
        "id": "uDyjUGpUiSsE"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Signals shape:\", signals.shape)  # Expected: (700, 1800, 2)\n",
        "print(\"Labels shape:\", labels.shape)    # Expected: (700,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DP4HSDS8iUHN",
        "outputId": "75f02659-277c-4c4e-9a80-ea692536b95c"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Signals shape: (700, 1800, 2)\n",
            "Labels shape: (700,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Define split ratios\n",
        "train_ratio = 0.8\n",
        "val_ratio = 0.1\n",
        "test_ratio = 0.1\n",
        "\n",
        "# Split into train and remaining (val + test)\n",
        "signals_train, signals_temp, labels_train, labels_temp = train_test_split(\n",
        "    signals, labels, test_size=(val_ratio + test_ratio), random_state=42, stratify=labels\n",
        ")\n",
        "\n",
        "# Split remaining into val and test\n",
        "signals_val, signals_test, labels_val, labels_test = train_test_split(\n",
        "    signals_temp, labels_temp, test_size=(test_ratio / (val_ratio + test_ratio)), random_state=42, stratify=labels_temp\n",
        ")\n",
        "\n",
        "# Print dataset sizes\n",
        "print(f\"Training set: {signals_train.shape}, Labels: {labels_train.shape}\")\n",
        "print(f\"Validation set: {signals_val.shape}, Labels: {labels_val.shape}\")\n",
        "print(f\"Test set: {signals_test.shape}, Labels: {labels_test.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lDCXnwttieBX",
        "outputId": "040d6947-39bf-4aeb-f367-df79a93d7d91"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set: (560, 1800, 2), Labels: (560,)\n",
            "Validation set: (70, 1800, 2), Labels: (70,)\n",
            "Test set: (70, 1800, 2), Labels: (70,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install imbalanced-learn\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TpJPj_y9ioSN",
        "outputId": "2f19de62-323f-4d31-f28e-9c36b280977a"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.26.4)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.5.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from imblearn.over_sampling import SMOTE\n"
      ],
      "metadata": {
        "id": "mR9Uh27SipzQ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class distribution before SMOTE\n",
        "unique, counts = np.unique(labels_train, return_counts=True)\n",
        "print(\"Class distribution before SMOTE:\", dict(zip(unique, counts)))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORWf7y0cisUk",
        "outputId": "6ab3787c-70c8-4e37-e098-519626150845"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution before SMOTE: {0: 374, 1: 186}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Flatten the data for SMOTE (SMOTE expects 2D input: (samples, features))\n",
        "X_train_flat = signals_train.reshape(signals_train.shape[0], -1)  # (560, 1800*2)\n",
        "y_train = labels_train\n",
        "\n",
        "# Apply SMOTE with a better strategy (match class 0 count)\n",
        "smote = SMOTE(sampling_strategy=1.0, random_state=42)  # Make both classes equal\n",
        "X_resampled, y_resampled = smote.fit_resample(X_train_flat, y_train)\n",
        "\n",
        "# Reshape back to original shape (samples, time_steps, leads)\n",
        "signals_train_resampled = X_resampled.reshape(-1, 1800, 2)\n",
        "\n",
        "# Print new class distribution\n",
        "unique_resampled, counts_resampled = np.unique(y_resampled, return_counts=True)\n",
        "print(\"Class distribution after SMOTE:\", dict(zip(unique_resampled, counts_resampled)))\n",
        "\n",
        "# Update labels_train with resampled labels\n",
        "labels_train_resampled = y_resampled\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_vdP7TQivll",
        "outputId": "27870de9-ff43-4bbe-945a-61fdbcd3406b"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution after SMOTE: {0: 374, 1: 374}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "x-ERh2a3Y6uO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3124afc7-7e5e-44cc-c2d8-d0acf622da5a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train tensor shape: torch.Size([748, 1800, 2]) torch.Size([748])\n",
            "Val tensor shape: torch.Size([70, 1800, 2]) torch.Size([70])\n",
            "Test tensor shape: torch.Size([70, 1800, 2]) torch.Size([70])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Convert data to PyTorch tensors\n",
        "X_train_tensor = torch.tensor(signals_train_resampled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(labels_train_resampled, dtype=torch.long)\n",
        "\n",
        "X_val_tensor = torch.tensor(signals_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(labels_val, dtype=torch.long)\n",
        "\n",
        "X_test_tensor = torch.tensor(signals_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(labels_test, dtype=torch.long)\n",
        "\n",
        "# Print shapes to verify\n",
        "print(\"Train tensor shape:\", X_train_tensor.shape, y_train_tensor.shape)\n",
        "print(\"Val tensor shape:\", X_val_tensor.shape, y_val_tensor.shape)\n",
        "print(\"Test tensor shape:\", X_test_tensor.shape, y_test_tensor.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create TensorDatasets\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n"
      ],
      "metadata": {
        "id": "edWLp8rwhvfO"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "X2rbn9zWl76a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ee8e03e-c4eb-42e2-ad5b-f3f82c0bda9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train batches: 24\n",
            "Validation batches: 3\n",
            "Test batches: 3\n"
          ]
        }
      ],
      "source": [
        "# Define batch size\n",
        "BATCH_SIZE = 32  # You can adjust this based on GPU memory\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# Print number of batches\n",
        "print(\"Train batches:\", len(train_loader))\n",
        "print(\"Validation batches:\", len(val_loader))\n",
        "print(\"Test batches:\", len(test_loader))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ✅ Convert already defined tensors for training, validation, and testing\n",
        "train_data, train_labels = X_train_tensor, y_train_tensor\n",
        "val_data, val_labels = X_val_tensor, y_val_tensor\n",
        "test_data, test_labels = X_test_tensor, y_test_tensor\n",
        "\n",
        "# ✅ Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(val_data, val_labels), batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(TensorDataset(test_data, test_labels), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ✅ Define the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LocalLeadAttention(input_size=256, num_classes=4).to(device)  # Updated input_size\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ✅ Function to downsample input from 1800 → 256\n",
        "def downsample_input(inputs, target_size=256):\n",
        "    return F.interpolate(inputs.permute(0, 2, 1), size=target_size, mode=\"linear\").permute(0, 2, 1)\n",
        "\n",
        "# ✅ Training & Validation Function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        # 🚀 Training Loop\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = downsample_input(inputs.to(device)), labels.to(device)  # Apply downsampling\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            all_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # ✅ Compute Training Metrics\n",
        "        train_acc = accuracy_score(all_labels, all_preds)\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # 🚀 Validation Loop\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds, val_labels_list = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = downsample_input(inputs.to(device)), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "                val_labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "        # ✅ Compute Validation Metrics\n",
        "        val_acc = accuracy_score(val_labels_list, val_preds)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        precision = precision_score(val_labels_list, val_preds, average='macro', zero_division=0)\n",
        "        recall = recall_score(val_labels_list, val_preds, average='macro', zero_division=0)\n",
        "        f1 = f1_score(val_labels_list, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "        # ✅ Print Progress\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
        "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# 🚀 Train the Model\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device=device)\n",
        "\n",
        "# ✅ Function to Evaluate Model on Test Set\n",
        "def evaluate_model(model, test_loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    test_preds, test_labels_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = downsample_input(inputs.to(device)), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            test_labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    # ✅ Compute Test Metrics\n",
        "    test_acc = accuracy_score(test_labels_list, test_preds)\n",
        "    precision = precision_score(test_labels_list, test_preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(test_labels_list, test_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(test_labels_list, test_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n🚀 Test Set Evaluation:\\n\"\n",
        "          f\"Accuracy: {test_acc:.4f}, Precision: {precision:.4f}, \"\n",
        "          f\"Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "# ✅ Run Evaluation on Test Set\n",
        "evaluate_model(trained_model, test_loader, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "vHKN3GlWmzxk",
        "outputId": "5381c8d8-19e0-4a53-a116-ee6e1a52501c"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Train Loss: 0.9474, Train Acc: 0.4639 | Val Loss: 0.7583, Val Acc: 0.3286, Precision: 0.1643, Recall: 0.5000, F1-score: 0.2473\n",
            "Epoch 2/20 - Train Loss: 0.5746, Train Acc: 0.7072 | Val Loss: 0.3155, Val Acc: 0.8571, Precision: 0.8381, Recall: 0.8381, F1-score: 0.8381\n",
            "Epoch 3/20 - Train Loss: 0.3128, Train Acc: 0.8797 | Val Loss: 0.3566, Val Acc: 0.8286, Precision: 0.8198, Recall: 0.8612, F1-score: 0.8214\n",
            "Epoch 4/20 - Train Loss: 0.2233, Train Acc: 0.9211 | Val Loss: 0.2754, Val Acc: 0.8714, Precision: 0.8542, Recall: 0.8932, F1-score: 0.8634\n",
            "Epoch 5/20 - Train Loss: 0.2218, Train Acc: 0.9184 | Val Loss: 0.1791, Val Acc: 0.9000, Precision: 0.8841, Recall: 0.8922, F1-score: 0.8879\n",
            "Epoch 6/20 - Train Loss: 0.1032, Train Acc: 0.9693 | Val Loss: 0.1660, Val Acc: 0.9429, Precision: 0.9289, Recall: 0.9463, F1-score: 0.9366\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-6e2c0e8d8387>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;31m# 🚀 Train the Model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;31m# ✅ Function to Evaluate Model on Test Set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-62-6e2c0e8d8387>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    579\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             )\n\u001b[0;32m--> 581\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    582\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m     _engine_run_backward(\n\u001b[0m\u001b[1;32m    348\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\u001b[0m in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m         \u001b[0munregister_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_register_logging_hooks_on_whole_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m         return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    826\u001b[0m             \u001b[0mt_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m         )  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# ✅ Convert already defined tensors for training, validation, and testing\n",
        "train_data, train_labels = X_train_tensor, y_train_tensor\n",
        "val_data, val_labels = X_val_tensor, y_val_tensor\n",
        "test_data, test_labels = X_test_tensor, y_test_tensor\n",
        "\n",
        "# ✅ Create DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(TensorDataset(train_data, train_labels), batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(val_data, val_labels), batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(TensorDataset(test_data, test_labels), batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# ✅ Define the model, loss function, and optimizer\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = LocalLeadAttention(input_size=256, num_classes=4).to(device)  # Updated input_size\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# ✅ Function to downsample input from 1800 → 256\n",
        "def downsample_input(inputs, target_size=256):\n",
        "    return F.interpolate(inputs.permute(0, 2, 1), size=target_size, mode=\"linear\").permute(0, 2, 1)\n",
        "\n",
        "# ✅ Training & Validation Function\n",
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device=\"cuda\"):\n",
        "    model.to(device)\n",
        "    best_val_acc = 0.0  # Track best validation accuracy\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        all_preds, all_labels = [], []\n",
        "\n",
        "        # 🚀 Training Loop\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = downsample_input(inputs.to(device)), labels.to(device)  # Apply downsampling\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            all_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "        # ✅ Compute Training Metrics\n",
        "        train_acc = accuracy_score(all_labels, all_preds)\n",
        "        avg_train_loss = train_loss / len(train_loader)\n",
        "\n",
        "        # 🚀 Validation Loop\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_preds, val_labels_list = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = downsample_input(inputs.to(device)), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                val_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "                val_labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "        # ✅ Compute Validation Metrics\n",
        "        val_acc = accuracy_score(val_labels_list, val_preds)\n",
        "        avg_val_loss = val_loss / len(val_loader)\n",
        "        precision = precision_score(val_labels_list, val_preds, average='macro', zero_division=0)\n",
        "        recall = recall_score(val_labels_list, val_preds, average='macro', zero_division=0)\n",
        "        f1 = f1_score(val_labels_list, val_preds, average='macro', zero_division=0)\n",
        "\n",
        "        # ✅ Save best model based on validation accuracy\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_model.pth\")\n",
        "            print(\"✅ Model saved!\")\n",
        "\n",
        "        # ✅ Print Progress\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.4f}, \"\n",
        "              f\"Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# 🚀 Train the Model\n",
        "trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs=20, device=device)\n",
        "\n",
        "# ✅ Load the Best Saved Model\n",
        "trained_model.load_state_dict(torch.load(\"best_model.pth\"))\n",
        "trained_model.eval()\n",
        "\n",
        "# ✅ Function to Evaluate Model on Test Set\n",
        "def evaluate_model(model, test_loader, device=\"cuda\"):\n",
        "    model.eval()\n",
        "    test_preds, test_labels_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = downsample_input(inputs.to(device)), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            test_preds.extend(torch.argmax(outputs, dim=1).cpu().numpy())\n",
        "            test_labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    # ✅ Compute Test Metrics\n",
        "    test_acc = accuracy_score(test_labels_list, test_preds)\n",
        "    precision = precision_score(test_labels_list, test_preds, average='macro', zero_division=0)\n",
        "    recall = recall_score(test_labels_list, test_preds, average='macro', zero_division=0)\n",
        "    f1 = f1_score(test_labels_list, test_preds, average='macro', zero_division=0)\n",
        "\n",
        "    print(f\"\\n🚀 Test Set Evaluation:\\n\"\n",
        "          f\"Accuracy: {test_acc:.4f}, Precision: {precision:.4f}, \"\n",
        "          f\"Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
        "\n",
        "# ✅ Run Evaluation on Test Set\n",
        "evaluate_model(trained_model, test_loader, device=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xi-HOwS2rvvr",
        "outputId": "dc396fe7-e34f-4381-afba-ef0595774237"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Model saved!\n",
            "Epoch 1/20 - Train Loss: 0.9649, Train Acc: 0.5067 | Val Loss: 0.5520, Val Acc: 0.7857, Precision: 0.7650, Recall: 0.7294, F1-score: 0.7413\n",
            "✅ Model saved!\n",
            "Epoch 2/20 - Train Loss: 0.5831, Train Acc: 0.6979 | Val Loss: 0.3613, Val Acc: 0.8429, Precision: 0.8207, Recall: 0.8275, F1-score: 0.8238\n",
            "✅ Model saved!\n",
            "Epoch 3/20 - Train Loss: 0.3760, Train Acc: 0.8436 | Val Loss: 0.2428, Val Acc: 0.9000, Precision: 0.8811, Recall: 0.9033, F1-score: 0.8901\n",
            "Epoch 4/20 - Train Loss: 0.2237, Train Acc: 0.9184 | Val Loss: 0.2136, Val Acc: 0.8857, Precision: 0.8656, Recall: 0.8927, F1-score: 0.8756\n",
            "✅ Model saved!\n",
            "Epoch 5/20 - Train Loss: 0.1793, Train Acc: 0.9225 | Val Loss: 0.1387, Val Acc: 0.9714, Precision: 0.9676, Recall: 0.9676, F1-score: 0.9676\n",
            "Epoch 6/20 - Train Loss: 0.1098, Train Acc: 0.9639 | Val Loss: 0.1787, Val Acc: 0.9143, Precision: 0.8958, Recall: 0.9251, F1-score: 0.9067\n",
            "Epoch 7/20 - Train Loss: 0.0791, Train Acc: 0.9719 | Val Loss: 0.1792, Val Acc: 0.9143, Precision: 0.8958, Recall: 0.9251, F1-score: 0.9067\n",
            "Epoch 8/20 - Train Loss: 0.0473, Train Acc: 0.9866 | Val Loss: 0.1670, Val Acc: 0.9429, Precision: 0.9289, Recall: 0.9463, F1-score: 0.9366\n",
            "Epoch 9/20 - Train Loss: 0.0563, Train Acc: 0.9826 | Val Loss: 0.1575, Val Acc: 0.9571, Precision: 0.9475, Recall: 0.9570, F1-score: 0.9520\n",
            "Epoch 10/20 - Train Loss: 0.0574, Train Acc: 0.9773 | Val Loss: 0.1466, Val Acc: 0.9714, Precision: 0.9676, Recall: 0.9676, F1-score: 0.9676\n",
            "Epoch 11/20 - Train Loss: 0.0493, Train Acc: 0.9826 | Val Loss: 0.1335, Val Acc: 0.9714, Precision: 0.9676, Recall: 0.9676, F1-score: 0.9676\n",
            "Epoch 12/20 - Train Loss: 0.0941, Train Acc: 0.9599 | Val Loss: 0.3372, Val Acc: 0.8714, Precision: 0.8542, Recall: 0.8932, F1-score: 0.8634\n",
            "Epoch 13/20 - Train Loss: 0.0716, Train Acc: 0.9759 | Val Loss: 0.1681, Val Acc: 0.9571, Precision: 0.9475, Recall: 0.9570, F1-score: 0.9520\n",
            "Epoch 14/20 - Train Loss: 0.0189, Train Acc: 0.9987 | Val Loss: 0.2244, Val Acc: 0.9571, Precision: 0.9475, Recall: 0.9570, F1-score: 0.9520\n",
            "Epoch 15/20 - Train Loss: 0.0247, Train Acc: 0.9947 | Val Loss: 0.1861, Val Acc: 0.9571, Precision: 0.9475, Recall: 0.9570, F1-score: 0.9520\n",
            "Epoch 16/20 - Train Loss: 0.0221, Train Acc: 0.9920 | Val Loss: 0.2208, Val Acc: 0.9429, Precision: 0.9289, Recall: 0.9463, F1-score: 0.9366\n",
            "Epoch 17/20 - Train Loss: 0.0121, Train Acc: 0.9973 | Val Loss: 0.1664, Val Acc: 0.9571, Precision: 0.9564, Recall: 0.9459, F1-score: 0.9509\n",
            "Epoch 18/20 - Train Loss: 0.0146, Train Acc: 0.9947 | Val Loss: 0.2204, Val Acc: 0.9429, Precision: 0.9289, Recall: 0.9463, F1-score: 0.9366\n",
            "Epoch 19/20 - Train Loss: 0.0078, Train Acc: 0.9973 | Val Loss: 0.2896, Val Acc: 0.9571, Precision: 0.9475, Recall: 0.9570, F1-score: 0.9520\n",
            "Epoch 20/20 - Train Loss: 0.0132, Train Acc: 0.9933 | Val Loss: 0.3360, Val Acc: 0.9143, Precision: 0.8958, Recall: 0.9251, F1-score: 0.9067\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-67-0ad0d5d6e4d5>:97: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  trained_model.load_state_dict(torch.load(\"best_model.pth\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🚀 Test Set Evaluation:\n",
            "Accuracy: 0.9571, Precision: 0.9570, Recall: 0.9475, F1-score: 0.9520\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}